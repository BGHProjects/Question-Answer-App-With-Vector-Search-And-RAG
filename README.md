# Question Answer App With Vector Search And RAG

</br>
<div align="center">
<a href="https://www.python.org/"><img src="./readme-content/Python.png" width="75" height="75"></a>
<a href="https://openai.com/"><img src="./readme-content/img/stacklogos/OpenAI.png" width="75" height="75"></a>
<a href="https://www.langchain.com/"><img src="./readme-content/img/stacklogos/Langchain.png" width="75" height="75"></a>
<a href="https://www.gradio.app/"><img src="./readme-content/img/stacklogos/Gradio.png" width="75" height="75"></a>
<a href="https://www.mongodb.com/atlas/database"><img src="./readme-content/img/stacklogos/MongoDB.png" width="75" height="75"></a>
</div>

</br>

# Overview

- This repository is adapted from [this tutorial](https://www.youtube.com/watch?v=JEBDfGqrAUA&t=2008s) from Freecodecamp regarding how to combine [vector search](https://www.elastic.co/what-is/vector-search) with [RAG](https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/) to provide natural language answers to user inputted questions
- The purpose of following this tutorial was to learn about both vectorisation and RAG, through building a practical application
- The content of this repository is split into two files; `load_data.py` and `extract_information.py`, which will be explained in detail below
- The following app requires an OpenAI API key and a MongoDB connection string, which have not been provided to this repository for security reasons

# Content

## `load_data.py`

This file establishes a connection to a MongoDB cluster using the pymongo library. It initializes a MongoClient instance with a MongoDB Uniform Resource Identifier (URI) retrieved from the 'key_param' configuration file. The code specifies the target database as 'langchain_demo' and the collection within the database as 'collection_of_text_blobs'. This MongoDB collection is then prepared for subsequent operations.

In the next section, the code employs the langchain library to load text data into the MongoDB collection. It uses the DirectoryLoader class, specifying the source directory as './sample_files' and the file pattern as '\*.txt'. The loader facilitates the import of text files into a data structure stored in the 'data' variable.

The code then focuses on natural language processing, generating embeddings for the loaded text data using OpenAI's Embeddings API. An instance of OpenAIEmbeddings is created with the OpenAI API key obtained from 'key_param'. This instance is utilized to generate embeddings for the text data. Subsequently, a MongoDBAtlasVectorSearch instance is created using the langchain library, named 'vectorStore'. This instance enables the execution of vector searches on the MongoDB collection based on the generated embeddings, providing the capability for advanced text-based queries and analyses within the MongoDB environment.

## `extract_information.py`

The initial section of this file establishes a connection to a MongoDB cluster using the MongoClient class, setting up a client instance with a MongoDB URI obtained from the 'key_param' configuration file. It then defines the target database ('langchain_demo') and collection ('collection_of_text_blobs'), preparing the MongoDB collection for further operations.

Following the MongoDB setup, the code initializes OpenAI embeddings using the OpenAIEmbeddings class from the langchain library. The embeddings are generated using the OpenAI API key specified in 'key_param'. Subsequently, a MongoDBAtlasVectorSearch instance is created, named 'vectorStore', to facilitate vector searches within the MongoDB collection based on the generated embeddings.

The core functionality of the code lies in the query_data function. This function takes a user query, transforms it into a vector, and performs a similarity search using the 'vectorStore'. It further utilizes langchain's capabilities to set up a retriever and question-answering model. The function returns the output generated by chaining Atlas Vector Search to langchain's RetrieverQA and OpenAI's LLM.

The code concludes with the definition of a Gradio UI, creating an interactive interface for users. The UI includes a text input box for user queries, a submit button, and a text output box to display the results of the question-answering process. The UI is structured using Gradio's Blocks and styled with a custom theme ('Base') to enhance the visual presentation.
